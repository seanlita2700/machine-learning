

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

/kaggle/input/titanic/train.csv
/kaggle/input/titanic/test.csv
/kaggle/input/titanic/gender_submission.csv

Import Modules

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')
%matplotlib inline

Loading the dataset

train = pd.read_csv('/kaggle/input/titanic/train.csv')
test = pd.read_csv('/kaggle/input/titanic/test.csv')
train.head()

	PassengerId 	Survived 	Pclass 	Name 	Sex 	Age 	SibSp 	Parch 	Ticket 	Fare 	Cabin 	Embarked
0 	1 	0 	3 	Braund, Mr. Owen Harris 	male 	22.0 	1 	0 	A/5 21171 	7.2500 	NaN 	S
1 	2 	1 	1 	Cumings, Mrs. John Bradley (Florence Briggs Th... 	female 	38.0 	1 	0 	PC 17599 	71.2833 	C85 	C
2 	3 	1 	3 	Heikkinen, Miss. Laina 	female 	26.0 	0 	0 	STON/O2. 3101282 	7.9250 	NaN 	S
3 	4 	1 	1 	Futrelle, Mrs. Jacques Heath (Lily May Peel) 	female 	35.0 	1 	0 	113803 	53.1000 	C123 	S
4 	5 	0 	3 	Allen, Mr. William Henry 	male 	35.0 	0 	0 	373450 	8.0500 	NaN 	S

## statistical info
train.describe()

	PassengerId 	Survived 	Pclass 	Age 	SibSp 	Parch 	Fare
count 	891.000000 	891.000000 	891.000000 	714.000000 	891.000000 	891.000000 	891.000000
mean 	446.000000 	0.383838 	2.308642 	29.699118 	0.523008 	0.381594 	32.204208
std 	257.353842 	0.486592 	0.836071 	14.526497 	1.102743 	0.806057 	49.693429
min 	1.000000 	0.000000 	1.000000 	0.420000 	0.000000 	0.000000 	0.000000
25% 	223.500000 	0.000000 	2.000000 	20.125000 	0.000000 	0.000000 	7.910400
50% 	446.000000 	0.000000 	3.000000 	28.000000 	0.000000 	0.000000 	14.454200
75% 	668.500000 	1.000000 	3.000000 	38.000000 	1.000000 	0.000000 	31.000000
max 	891.000000 	1.000000 	3.000000 	80.000000 	8.000000 	6.000000 	512.329200

## datatype info
train.info()

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 891 entries, 0 to 890
Data columns (total 12 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   PassengerId  891 non-null    int64  
 1   Survived     891 non-null    int64  
 2   Pclass       891 non-null    int64  
 3   Name         891 non-null    object 
 4   Sex          891 non-null    object 
 5   Age          714 non-null    float64
 6   SibSp        891 non-null    int64  
 7   Parch        891 non-null    int64  
 8   Ticket       891 non-null    object 
 9   Fare         891 non-null    float64
 10  Cabin        204 non-null    object 
 11  Embarked     889 non-null    object 
dtypes: float64(2), int64(5), object(5)
memory usage: 83.7+ KB

Exploratory Data Analysis

## categorical attributes
sns.countplot(train['Survived'])

<AxesSubplot:xlabel='Survived', ylabel='count'>

sns.countplot(train['Pclass'])

<AxesSubplot:xlabel='Pclass', ylabel='count'>

sns.countplot(train['Sex'])

<AxesSubplot:xlabel='Sex', ylabel='count'>

sns.countplot(train['SibSp'])

<AxesSubplot:xlabel='SibSp', ylabel='count'>

sns.countplot(train['Parch'])

<AxesSubplot:xlabel='Parch', ylabel='count'>

sns.countplot(train['Embarked'])

<AxesSubplot:xlabel='Embarked', ylabel='count'>

## numerical attributes
sns.distplot(train['Age'])

<AxesSubplot:xlabel='Age', ylabel='Density'>

sns.distplot(train['Fare'])

<AxesSubplot:xlabel='Fare', ylabel='Density'>

class_fare = train.pivot_table(index='Pclass', values='Fare')
class_fare.plot(kind='bar')
plt.xlabel('Pclass')
plt.ylabel('Avg. Fare')
plt.xticks(rotation=0)
plt.show()

class_fare = train.pivot_table(index='Pclass', values='Fare', aggfunc=np.sum)
class_fare.plot(kind='bar')
plt.xlabel('Pclass')
plt.ylabel('Total Fare')
plt.xticks(rotation=0)
plt.show()

sns.barplot(data=train, x='Pclass', y='Fare', hue='Survived')

<AxesSubplot:xlabel='Pclass', ylabel='Fare'>

sns.barplot(data=train, x='Survived', y='Fare', hue='Pclass')

<AxesSubplot:xlabel='Survived', ylabel='Fare'>

Data Preprocessing

train_len = len(train)
# combine two dataframes
df = pd.concat([train, test], axis=0)
df = df.reset_index(drop=True)
df.head()

	PassengerId 	Survived 	Pclass 	Name 	Sex 	Age 	SibSp 	Parch 	Ticket 	Fare 	Cabin 	Embarked
0 	1 	0.0 	3 	Braund, Mr. Owen Harris 	male 	22.0 	1 	0 	A/5 21171 	7.2500 	NaN 	S
1 	2 	1.0 	1 	Cumings, Mrs. John Bradley (Florence Briggs Th... 	female 	38.0 	1 	0 	PC 17599 	71.2833 	C85 	C
2 	3 	1.0 	3 	Heikkinen, Miss. Laina 	female 	26.0 	0 	0 	STON/O2. 3101282 	7.9250 	NaN 	S
3 	4 	1.0 	1 	Futrelle, Mrs. Jacques Heath (Lily May Peel) 	female 	35.0 	1 	0 	113803 	53.1000 	C123 	S
4 	5 	0.0 	3 	Allen, Mr. William Henry 	male 	35.0 	0 	0 	373450 	8.0500 	NaN 	S

df.tail()

	PassengerId 	Survived 	Pclass 	Name 	Sex 	Age 	SibSp 	Parch 	Ticket 	Fare 	Cabin 	Embarked
1304 	1305 	NaN 	3 	Spector, Mr. Woolf 	male 	NaN 	0 	0 	A.5. 3236 	8.0500 	NaN 	S
1305 	1306 	NaN 	1 	Oliva y Ocana, Dona. Fermina 	female 	39.0 	0 	0 	PC 17758 	108.9000 	C105 	C
1306 	1307 	NaN 	3 	Saether, Mr. Simon Sivertsen 	male 	38.5 	0 	0 	SOTON/O.Q. 3101262 	7.2500 	NaN 	S
1307 	1308 	NaN 	3 	Ware, Mr. Frederick 	male 	NaN 	0 	0 	359309 	8.0500 	NaN 	S
1308 	1309 	NaN 	3 	Peter, Master. Michael J 	male 	NaN 	1 	1 	2668 	22.3583 	NaN 	C

## find the null values
df.isnull().sum()

PassengerId       0
Survived        418
Pclass            0
Name              0
Sex               0
Age             263
SibSp             0
Parch             0
Ticket            0
Fare              1
Cabin          1014
Embarked          2
dtype: int64

# drop or delete the column
df = df.drop(columns=['Cabin'], axis=1)

df['Age'].mean()

29.881137667304014

# fill missing values using mean of the numerical column
df['Age'] = df['Age'].fillna(df['Age'].mean())
df['Fare'] = df['Fare'].fillna(df['Fare'].mean())

df['Embarked'].mode()[0]

'S'

# fill missing values using mode of the categorical column
df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])

Log transformation for uniform data distribution

sns.distplot(df['Fare'])

<AxesSubplot:xlabel='Fare', ylabel='Density'>

df['Fare'] = np.log(df['Fare']+1)

sns.distplot(df['Fare'])

<AxesSubplot:xlabel='Fare', ylabel='Density'>

Correlation Matrix

corr = df.corr()
plt.figure(figsize=(15, 9))
sns.heatmap(corr, annot=True, cmap='coolwarm')

<AxesSubplot:>

df.head()

	PassengerId 	Survived 	Pclass 	Name 	Sex 	Age 	SibSp 	Parch 	Ticket 	Fare 	Embarked
0 	1 	0.0 	3 	Braund, Mr. Owen Harris 	male 	22.0 	1 	0 	A/5 21171 	2.110213 	S
1 	2 	1.0 	1 	Cumings, Mrs. John Bradley (Florence Briggs Th... 	female 	38.0 	1 	0 	PC 17599 	4.280593 	C
2 	3 	1.0 	3 	Heikkinen, Miss. Laina 	female 	26.0 	0 	0 	STON/O2. 3101282 	2.188856 	S
3 	4 	1.0 	1 	Futrelle, Mrs. Jacques Heath (Lily May Peel) 	female 	35.0 	1 	0 	113803 	3.990834 	S
4 	5 	0.0 	3 	Allen, Mr. William Henry 	male 	35.0 	0 	0 	373450 	2.202765 	S

## drop unnecessary columns
df = df.drop(columns=['Name', 'Ticket'], axis=1)
df.head()

	PassengerId 	Survived 	Pclass 	Sex 	Age 	SibSp 	Parch 	Fare 	Embarked
0 	1 	0.0 	3 	male 	22.0 	1 	0 	2.110213 	S
1 	2 	1.0 	1 	female 	38.0 	1 	0 	4.280593 	C
2 	3 	1.0 	3 	female 	26.0 	0 	0 	2.188856 	S
3 	4 	1.0 	1 	female 	35.0 	1 	0 	3.990834 	S
4 	5 	0.0 	3 	male 	35.0 	0 	0 	2.202765 	S
Label Encoding

from sklearn.preprocessing import LabelEncoder
cols = ['Sex', 'Embarked']
le = LabelEncoder()

for col in cols:
    df[col] = le.fit_transform(df[col])
df.head()

	PassengerId 	Survived 	Pclass 	Sex 	Age 	SibSp 	Parch 	Fare 	Embarked
0 	1 	0.0 	3 	1 	22.0 	1 	0 	2.110213 	2
1 	2 	1.0 	1 	0 	38.0 	1 	0 	4.280593 	0
2 	3 	1.0 	3 	0 	26.0 	0 	0 	2.188856 	2
3 	4 	1.0 	1 	0 	35.0 	1 	0 	3.990834 	2
4 	5 	0.0 	3 	1 	35.0 	0 	0 	2.202765 	2
Train-Test Split

train = df.iloc[:train_len, :]
test = df.iloc[train_len:, :]

train.head()

	PassengerId 	Survived 	Pclass 	Sex 	Age 	SibSp 	Parch 	Fare 	Embarked
0 	1 	0.0 	3 	1 	22.0 	1 	0 	2.110213 	2
1 	2 	1.0 	1 	0 	38.0 	1 	0 	4.280593 	0
2 	3 	1.0 	3 	0 	26.0 	0 	0 	2.188856 	2
3 	4 	1.0 	1 	0 	35.0 	1 	0 	3.990834 	2
4 	5 	0.0 	3 	1 	35.0 	0 	0 	2.202765 	2

test.head()

	PassengerId 	Survived 	Pclass 	Sex 	Age 	SibSp 	Parch 	Fare 	Embarked
891 	892 	NaN 	3 	1 	34.5 	0 	0 	2.178064 	1
892 	893 	NaN 	3 	0 	47.0 	1 	0 	2.079442 	2
893 	894 	NaN 	2 	1 	62.0 	0 	0 	2.369075 	1
894 	895 	NaN 	3 	1 	27.0 	0 	0 	2.268252 	2
895 	896 	NaN 	3 	0 	22.0 	1 	1 	2.586824 	2

# input split
X = train.drop(columns=['PassengerId', 'Survived'], axis=1)
y = train['Survived']

X.head()

	Pclass 	Sex 	Age 	SibSp 	Parch 	Fare 	Embarked
0 	3 	1 	22.0 	1 	0 	2.110213 	2
1 	1 	0 	38.0 	1 	0 	4.280593 	0
2 	3 	0 	26.0 	0 	0 	2.188856 	2
3 	1 	0 	35.0 	1 	0 	3.990834 	2
4 	3 	1 	35.0 	0 	0 	2.202765 	2
Model Training

from sklearn.model_selection import train_test_split, cross_val_score
# classify column
def classify(model):
    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)
    model.fit(x_train, y_train)
    print('Accuracy:', model.score(x_test, y_test))
    
    score = cross_val_score(model, X, y, cv=5)
    print('CV Score:', np.mean(score))

from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
classify(model)

Accuracy: 0.8071748878923767
CV Score: nan

from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier()
classify(model)

Accuracy: 0.7174887892376681
CV Score: 0.7744460485845208

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier()
classify(model)

Accuracy: 0.8026905829596412
CV Score: 0.8081036971941498

from sklearn.ensemble import ExtraTreesClassifier
model = ExtraTreesClassifier()
classify(model)

Accuracy: 0.7937219730941704
CV Score: 0.7890465130876907

from xgboost import XGBClassifier
model = XGBClassifier()
classify(model)

[14:40:05] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
Accuracy: 0.7892376681614349
[14:40:05] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
[14:40:05] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
[14:40:05] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
[14:40:05] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
[14:40:05] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
CV Score: 0.8125980792166217

from lightgbm import LGBMClassifier
model = LGBMClassifier()
classify(model)

Accuracy: 0.8116591928251121
CV Score: 0.8238277572029377

from catboost import CatBoostClassifier
model = CatBoostClassifier(verbose=0)
classify(model)

Accuracy: 0.8295964125560538
CV Score: 0.8226790534178645

Complete Model Training with Full Data

model = LGBMClassifier()
model.fit(X, y)

LGBMClassifier()

test.head()

	PassengerId 	Survived 	Pclass 	Sex 	Age 	SibSp 	Parch 	Fare 	Embarked
891 	892 	NaN 	3 	1 	34.5 	0 	0 	2.178064 	1
892 	893 	NaN 	3 	0 	47.0 	1 	0 	2.079442 	2
893 	894 	NaN 	2 	1 	62.0 	0 	0 	2.369075 	1
894 	895 	NaN 	3 	1 	27.0 	0 	0 	2.268252 	2
895 	896 	NaN 	3 	0 	22.0 	1 	1 	2.586824 	2

# input split for test data
X_test = test.drop(columns=['PassengerId', 'Survived'], axis=1)

X_test.head()

	Pclass 	Sex 	Age 	SibSp 	Parch 	Fare 	Embarked
891 	3 	1 	34.5 	0 	0 	2.178064 	1
892 	3 	0 	47.0 	1 	0 	2.079442 	2
893 	2 	1 	62.0 	0 	0 	2.369075 	1
894 	3 	1 	27.0 	0 	0 	2.268252 	2
895 	3 	0 	22.0 	1 	1 	2.586824 	2

pred = model.predict(X_test)
pred

array([0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0.,
       1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
       1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1.,
       0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0.,
       0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0.,
       0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,
       0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1.,
       1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,
       0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0.,
       0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1.,
       0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1.,
       0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1.,
       0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,
       1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,
       0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
       0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
       1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
       0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1.,
       0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
       1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
       1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
       1., 1., 1., 1., 0., 0., 1., 0., 0., 1.])

Test Submission

sub = pd.read_csv('/kaggle/input/titanic/gender_submission.csv')
sub.head()

	PassengerId 	Survived
0 	892 	0
1 	893 	1
2 	894 	0
3 	895 	0
4 	896 	1

sub.info()

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 418 entries, 0 to 417
Data columns (total 2 columns):
 #   Column       Non-Null Count  Dtype
---  ------       --------------  -----
 0   PassengerId  418 non-null    int64
 1   Survived     418 non-null    int64
dtypes: int64(2)
memory usage: 6.7 KB

sub['Survived'] = pred
sub['Survived'] = sub['Survived'].astype('int')

sub.info()

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 418 entries, 0 to 417
Data columns (total 2 columns):
 #   Column       Non-Null Count  Dtype
---  ------       --------------  -----
 0   PassengerId  418 non-null    int64
 1   Survived     418 non-null    int64
dtypes: int64(2)
memory usage: 6.7 KB

sub.head()

	PassengerId 	Survived
0 	892 	0
1 	893 	0
2 	894 	0
3 	895 	1
4 	896 	0

sub.to_csv('submission.csv', index=False)

 

 

 

